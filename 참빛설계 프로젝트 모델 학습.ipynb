{"cells":[{"cell_type":"markdown","source":["<h3> 1. 필요한 라이브러리 설치 </h3>"],"metadata":{"id":"Et-slrulYSC5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtEwSJldzbdp"},"outputs":[],"source":["!pip install konlpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BznYBlDEaAhl"},"outputs":[],"source":["!sudo apt-get install -y fonts-nanum\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IaW_3TqzAJ0"},"outputs":[],"source":["!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n","%cd Mecab-ko-for-Google-Colab\n","!bash install_mecab-ko_on_colab190912.sh"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24939,"status":"ok","timestamp":1691930954084,"user":{"displayName":"cel ee","userId":"14998685299353417937"},"user_tz":-540},"id":"A0oH6bWC3m3V","outputId":"b90c888c-43ee-4b88-9613-00e5af718fad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["<h3> 2. 파일 불러오기 </h3>\n","<li> 토크나이징이 완료된 경우에는 pickle 파일 불러오기 </li>\n","<li> 토크나이징이 되지 않은 경우에는 AI Hub 원본 데이터 불러오기 </li>"],"metadata":{"id":"pp_y1pieYUpe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"88dRKlSQy-j1"},"outputs":[],"source":["import pickle\n","import pandas as pd\n","## train data가 존재할 경우 저장해놓은 피클 불러오기\n","df = pd.read_pickle('/content/drive/MyDrive/train.pkl')\n","df[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"executionInfo":{"elapsed":19189,"status":"ok","timestamp":1691939971782,"user":{"displayName":"cel ee","userId":"14998685299353417937"},"user_tz":-540},"id":"8oR_zD7fC9n2","outputId":"16b6ec91-9f3c-4c05-a2e6-9a75111c781d"},"outputs":[{"data":{"text/html":["\n","\n","  <div id=\"df-56fe6d81-d1ae-44f1-abaf-ca5f8c0ff3fa\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>번호</th>\n","      <th>value</th>\n","      <th>연령</th>\n","      <th>성별</th>\n","      <th>상황키워드</th>\n","      <th>신체질환</th>\n","      <th>감정_대분류</th>\n","      <th>감정_소분류</th>\n","      <th>사람문장1</th>\n","      <th>시스템응답1</th>\n","      <th>사람문장2</th>\n","      <th>시스템응답2</th>\n","      <th>사람문장3</th>\n","      <th>시스템응답3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>24070</td>\n","      <td>1</td>\n","      <td>청년</td>\n","      <td>여성</td>\n","      <td>진로, 취업, 직장</td>\n","      <td>해당없음</td>\n","      <td>기쁨</td>\n","      <td>신이 난</td>\n","      <td>지금 난 기분이 너무 좋아.</td>\n","      <td>굉장히 즐거우신 것 같은데요? 좋은 일이 있나요?</td>\n","      <td>오늘부터 연휴가 시작됐거든. 내일 회사 안 간다.</td>\n","      <td>그거 정말 기분 좋을 만하네요. 무엇을 할 예정인가요?</td>\n","      <td>그냥 집에서 뒹굴뒹굴하고 넷플릭스 보고 놀 거야.</td>\n","      <td>집에서 노는 것이 최고죠. 생각만 해도 행복하겠어요.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>27879</td>\n","      <td>1</td>\n","      <td>청년</td>\n","      <td>여성</td>\n","      <td>연애, 결혼, 출산</td>\n","      <td>해당없음</td>\n","      <td>기쁨</td>\n","      <td>신이 난</td>\n","      <td>나도 조카가 생겨! 너무 기뻐.</td>\n","      <td>정말 좋은 소식이네요. 축하할 특별한 계획이 있을까요?</td>\n","      <td>응. 언니에게 꽃다발을 선물할 거야. 예쁜 딸이면 좋을 것 같아서.</td>\n","      <td>언니에게 꽃을 선물하려고 하시는군요.</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>51471</td>\n","      <td>1</td>\n","      <td>중년</td>\n","      <td>여성</td>\n","      <td>직장, 업무 스트레스</td>\n","      <td>해당없음</td>\n","      <td>불안</td>\n","      <td>불안</td>\n","      <td>거래처와의 다음 계약이 무산될까봐 불안해.</td>\n","      <td>많이 걱정되시겠어요. 계약이 무산될 것 같다고 생각하는 이유가 있으신가요?</td>\n","      <td>딱히 이유는 없어. 그냥 나 혼자 불안한 상태야.</td>\n","      <td>계약을 성공적으로 마무리하실 수 있기를 기원해요.</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7140</td>\n","      <td>1</td>\n","      <td>노년</td>\n","      <td>남성</td>\n","      <td>대인관계</td>\n","      <td>만성질환 유</td>\n","      <td>슬픔</td>\n","      <td>염세적인</td>\n","      <td>당뇨로 고생 중인데 아내가 이혼하자더군. 아내가 날 버리면 병은 더 악화될 텐데 세...</td>\n","      <td>몸이 아픈데 아내가 이혼하자고 하니 혼자 있는 것 같은 기분을 느끼셨군요.</td>\n","      <td>젊은 시절에 가정에 소홀했지. 그래도 그렇지 이혼을 하자니 너무 허망해.</td>\n","      <td>지금의 기분에서 벗어나기 위해 무엇을 하면 좋을까요?</td>\n","      <td>아내에게 졸혼은 어떤지 대화를 나누어 봐야겠어. 졸혼을 한다면 가끔씩이라도 도움을 ...</td>\n","      <td>아내와의 대화로 기분이 나아졌으면 좋겠어요.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>39663</td>\n","      <td>1</td>\n","      <td>노년</td>\n","      <td>남성</td>\n","      <td>재정</td>\n","      <td>만성질환 유</td>\n","      <td>슬픔</td>\n","      <td>마비된</td>\n","      <td>노년에 재정적으로 이루어 놓은게 없어서 걱정이야.</td>\n","      <td>나이들어 재정적으로 어려워서 걱정되시는군요.</td>\n","      <td>정말이지 막막해.</td>\n","      <td>막막한 현실이지만 분명 해결책을 찾을 수 있을거예요.</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56fe6d81-d1ae-44f1-abaf-ca5f8c0ff3fa')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-37b56c43-cfdf-469a-ad37-ecaa7a0c8f5c\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-37b56c43-cfdf-469a-ad37-ecaa7a0c8f5c')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-37b56c43-cfdf-469a-ad37-ecaa7a0c8f5c button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-56fe6d81-d1ae-44f1-abaf-ca5f8c0ff3fa button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-56fe6d81-d1ae-44f1-abaf-ca5f8c0ff3fa');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"],"text/plain":["      번호  value  연령  성별        상황키워드    신체질환 감정_대분류 감정_소분류  \\\n","0  24070      1  청년  여성   진로, 취업, 직장    해당없음     기쁨   신이 난   \n","1  27879      1  청년  여성   연애, 결혼, 출산    해당없음     기쁨   신이 난   \n","2  51471      1  중년  여성  직장, 업무 스트레스    해당없음     불안     불안   \n","3   7140      1  노년  남성         대인관계  만성질환 유     슬픔   염세적인   \n","4  39663      1  노년  남성           재정  만성질환 유     슬픔    마비된   \n","\n","                                               사람문장1  \\\n","0                                    지금 난 기분이 너무 좋아.   \n","1                                  나도 조카가 생겨! 너무 기뻐.   \n","2                            거래처와의 다음 계약이 무산될까봐 불안해.   \n","3  당뇨로 고생 중인데 아내가 이혼하자더군. 아내가 날 버리면 병은 더 악화될 텐데 세...   \n","4                        노년에 재정적으로 이루어 놓은게 없어서 걱정이야.   \n","\n","                                      시스템응답1  \\\n","0                굉장히 즐거우신 것 같은데요? 좋은 일이 있나요?   \n","1             정말 좋은 소식이네요. 축하할 특별한 계획이 있을까요?   \n","2  많이 걱정되시겠어요. 계약이 무산될 것 같다고 생각하는 이유가 있으신가요?   \n","3  몸이 아픈데 아내가 이혼하자고 하니 혼자 있는 것 같은 기분을 느끼셨군요.   \n","4                   나이들어 재정적으로 어려워서 걱정되시는군요.   \n","\n","                                      사람문장2                          시스템응답2  \\\n","0               오늘부터 연휴가 시작됐거든. 내일 회사 안 간다.  그거 정말 기분 좋을 만하네요. 무엇을 할 예정인가요?   \n","1     응. 언니에게 꽃다발을 선물할 거야. 예쁜 딸이면 좋을 것 같아서.            언니에게 꽃을 선물하려고 하시는군요.   \n","2               딱히 이유는 없어. 그냥 나 혼자 불안한 상태야.     계약을 성공적으로 마무리하실 수 있기를 기원해요.   \n","3  젊은 시절에 가정에 소홀했지. 그래도 그렇지 이혼을 하자니 너무 허망해.   지금의 기분에서 벗어나기 위해 무엇을 하면 좋을까요?   \n","4                                 정말이지 막막해.   막막한 현실이지만 분명 해결책을 찾을 수 있을거예요.   \n","\n","                                               사람문장3  \\\n","0                        그냥 집에서 뒹굴뒹굴하고 넷플릭스 보고 놀 거야.   \n","1                                                NaN   \n","2                                                NaN   \n","3  아내에게 졸혼은 어떤지 대화를 나누어 봐야겠어. 졸혼을 한다면 가끔씩이라도 도움을 ...   \n","4                                                NaN   \n","\n","                          시스템응답3  \n","0  집에서 노는 것이 최고죠. 생각만 해도 행복하겠어요.  \n","1                            NaN  \n","2                            NaN  \n","3       아내와의 대화로 기분이 나아졌으면 좋겠어요.  \n","4                            NaN  "]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import urllib.request\n","from collections import Counter\n","from konlpy.tag import Mecab\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm import tqdm\n","from konlpy.tag import Okt\n","import os\n","from scipy.sparse import save_npz, load_npz\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import re\n","\n","plt.rc('font', family='Malgun Gothic')\n","train_df = pd.read_excel('/content/drive/MyDrive/sentiment_analysis.xlsx')\n","train_df.head()"]},{"cell_type":"markdown","source":["데이터 중에서 필요한 컬럼(대화 뭉치)만 이용(사람문장, 시스템응답)"],"metadata":{"id":"Z0xnLk50Ypz2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jc8lhS31YDdD"},"outputs":[],"source":["train_df.fillna('', inplace = True)\n","train_df.drop(['번호', 'value' ,'연령', '성별', '상황키워드', '신체질환', '감정_소분류'],axis=1,inplace = True)"]},{"cell_type":"code","source":["### 감성분석 성능 실험 위해 감정을 3가지로 분류해보는 것도 실험 ###\n","#### 실제로는 감정_대분류를 예측 label로 하여 설정함 ####\n","sentimential_list = {'행복' : 0, '기쁨': 0, '분노' : 1, '불안' : 1, '당황' : 2, '슬픔' : 2, '상처' : 2}\n","train_df['label'] = train_df['감정_대분류'].map(sentimential_list)\n","train_df.head()"],"metadata":{"id":"wCbChMo1Y1eq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"76xgA3jlFwaH"},"source":["텍스트 전처리 : text를 모든 문장을 합쳐서 처리<br>\n","<p> 사람문장과 시스템응답 문장들을 모두 합쳐서 하나의 text로 설정\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMldvWwD4Cl2"},"outputs":[],"source":["train_df['text'] = train_df['사람문장1'] + ' ' + train_df['시스템응답1']  + ' ' + train_df['사람문장2'].map(str) + ' ' + train_df['시스템응답2']  + ' ' +train_df['사람문장3']  + ' ' + train_df['시스템응답3']\n","train_df['text'] = train_df['text'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n","train_df.drop_duplicates(subset = ['text'], inplace = True) #중복 제거"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1691939973090,"user":{"displayName":"cel ee","userId":"14998685299353417937"},"user_tz":-540},"id":"k8zpiPsmFDdF","outputId":"9af80e85-d29e-4840-fb22-136a92cd4879"},"outputs":[{"name":"stdout","output_type":"stream","text":["156.32599047849592\n"]}],"source":["print(sum(map(len, train_df['text'])) / len(train_df['text'])) # 평균 글자 수"]},{"cell_type":"markdown","source":["<h3> 3. Data Augmentation </h3>\n","<li> KAIST에서 만든 Korean WordNet을 이용하여 Data Augmentation 시도 </li>\n","<li> Data Augmentation 구성 : 문장 순서 바꾸기, 문장 단어 유사한 단어로 바꾸기, 단어 추가 & 제거 </li>\n","<li> 결론적으로 성능이 높아지기는 했지만, train data에 너무 과적합되어서 실제 LSTM 모델 구축 시에는 포함하지 않음 </li>"],"metadata":{"id":"DINWsm9cZE8r"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IP2HEncxM4Kf"},"outputs":[],"source":["import random\n","import pickle\n","import re\n","\n","wordnet = {}\n","with open(\"/content/drive/MyDrive/wordnet.pickle\", \"rb\") as f:\n","\twordnet = pickle.load(f)\n","\n"," # 한글만 남기고 나머지는 삭제\n","def get_only_hangul(line):\n","\tparseText= re.compile('/ ^[ㄱ-ㅎㅏ-ㅣ가-힣]*$/').sub('',str(line))\n","\n","\treturn parseText\n","\n","########################################################################\n","# Synonym replacement\n","# wordnet에 유사한 단어가 존재하면 그 단어를 return\n","########################################################################\n","def get_synonyms(word):\n","\tsynomyms = []\n","\ttry:\n","\t\tfor syn in wordnet[word]:\n","\t\t\tsynomyms.append(syn)\n","\texcept:\n","\t\tpass\n","\n","\treturn synomyms\n","\n","########################################################################\n","# Synonym replacement\n","# wordnet에 단어가 존재하는 경우에 유사한 단어로 바꿔줌\n","########################################################################\n","def synonym_replacement(words):\n","  new_words = words.copy()\n","  random_word_list = list(set([word for word in words]))\n","  random.shuffle(random_word_list)\n","  num_replaced = 0\n","  sentence = \"\"\n","  for random_word in random_word_list:\n","    synonyms = get_synonyms(random_word) ## 유사한 단어 추출\n","    if len(synonyms) >= 1: ## 유사한 단어가 있으면 유사한 단어로 바꿔준 문장을 추가함\n","      synonym = list(synonyms)\n","      new_words = [synonym[random.randrange(len(synonyms))] if word == random_word else word for word in new_words]\n","      sentence += ' '.join(new_words) + \"\\n\"\n","      num_replaced += 1\n","\n","\n","  return sentence\n","\n","## Data Augmentation을 본격적으로 진행하는 단계\n","def EDA(sentence, alpha_sr = 0.1, num_aug = 4):\n","  ## 형태소 분석기 선언\n","  okt = Okt()\n","  sentence = get_only_hangul(sentence)\n","  morphs = okt.morphs(sentence)\n","  num_of_morphs = len(morphs) ## 분할된 형태소 개수\n","\n","  augmented_sentences = []\n","  num_new_per_technique = num_aug\n","\n","  ## 한 문장당 augmentation을 진행할 개수 (4)\n","  for i in range(num_new_per_technique):\n","    a_words = synonym_replacement(morphs)\n","    augmented_sentences = a_words.split('\\n') ## augmented된 sentence 반환\n","\n","    augmented_sentences = [get_only_hangul(sentence) for sentence in augmented_sentences]\n","\n","    augmented_sentences.append(sentence)\n","    return [sentence for sentence in list(set(augmented_sentences)) if sentence is not \"\"]"]},{"cell_type":"markdown","source":["<h3> 3-1. Data Augmentation 없이 Okt로 토큰화 진행 </h3>"],"metadata":{"id":"QiJ-Y2CKavnz"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":294,"status":"ok","timestamp":1691939982432,"user":{"displayName":"cel ee","userId":"14998685299353417937"},"user_tz":-540},"id":"wueDgHcyMTw4","outputId":"05a20073-9e94-4fb9-96b7-f0653691099a"},"outputs":[{"data":{"text/plain":["((55768,), (18590,), (55768,), (18590,))"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["x_data = train_df['text']\n","y_data = train_df['감정_대분류']\n","\n","X_train_data, X_test_data, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.25, stratify = y_data, random_state = 42)\n","X_train_data.shape, X_test_data.shape, y_train.shape, y_test.shape"]},{"cell_type":"markdown","source":["시간이 매우 오래걸리기 때문에, 처음에 okt 진행 시에 pickle 파일로 저장. 나중에 처음 시작할 때 파일 경로 불러오면 됌"],"metadata":{"id":"niyndsS_a6rO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sr24WsQQOAzR"},"outputs":[],"source":["#불용어 처리\n","import pickle\n","okt = Okt()\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다', '것','적', '내', '때'] ## 불용어 제거\n","\n","X_train = []\n","X_test = []\n","\n","########################################################################\n","# 형태소 분석기 이용하여 토크나이징 진행\n","# 토크나이징 된 것을 pickle 파일로 저장\n","########################################################################\n","\n","if not os.path.isfile('/content/drive/MyDrive/train.pkl'):\n","  for sentence in tqdm(X_train_data):\n","    tokenized_sentence = okt.morphs(sentence, stem = True)\n","    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n","    X_train.append(stopwords_removed_sentence)\n","  with open(\"/content/drive/MyDrive/train.pkl\",\"wb\") as f:\n","    pickle.dump(X_train, f)\n","  for sentence in tqdm(X_test_data):\n","    tokenized_sentence = okt.morphs(sentence, stem = True)\n","    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n","    X_test.append(stopwords_removed_sentence)\n","  with open(\"/content/drive/MyDrive/test.pkl\",\"wb\") as f:\n","      pickle.dump(X_test, f)\n","else:\n","    with open(\"/content/drive/MyDrive/train.pkl\",\"rb\") as f:\n","        X_train = pickle.load(f)\n","    with open(\"/content/drive/MyDrive/test.pkl\", \"rb\") as f:\n","        X_test = pickle.load(f)"]},{"cell_type":"markdown","source":["<h3> 3-2. 텍스트를 벡터화 진행하기 </h3>"],"metadata":{"id":"FStswGFucctI"}},{"cell_type":"markdown","source":["<li> 토크나이징 된 결과의 형태를 숫자로 표현하기\n","ex. '슬프다' -> 1, '기쁘다' -> 2와 같이 단어를 숫자로 바꾸기 </li>\n","<li> 등장빈도가 적은 희귀 단어를 제외하고 학습하기 -> 총 3758개의 단어</li>"],"metadata":{"id":"8B3LZeEmblJJ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1232,"status":"ok","timestamp":1691939990894,"user":{"displayName":"cel ee","userId":"14998685299353417937"},"user_tz":-540},"id":"TIEwfFvnGqFK","outputId":"fad19fd7-fcf6-4867-abf5-e6e84157808d"},"outputs":[{"name":"stdout","output_type":"stream","text":["단어 집합(vocabulary)의 크기 : 17297\n","등장 빈도가 29번 이하인 희귀 단어의 수: 13540\n","단어 집합에서 희귀 단어의 비율: 78.27947042839799\n","전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 2.9549672911589573\n","학습된 총 단어의 크기 :  3758\n"]}],"source":["########################################################################\n","# 토크나이징 된 결과를 이용하여 단어를 숫자로 바꾸는 과정\n","# 희귀 단어 제외하고 학습하기\n","########################################################################\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train)\n","\n","threshold = 30\n","total_cnt = len(tokenizer.word_index)\n","rare_cnt = 0\n","total_freq = 0\n","rare_freq = 0\n","\n","for key, value in tokenizer.word_counts.items():\n","  total_freq = total_freq + value\n","  if(value < threshold):\n","    rare_cnt = rare_cnt + 1\n","    rare_freq = rare_freq + value\n","\n","print('단어 집합(vocabulary)의 크기 :',total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n","\n","vocab_size = total_cnt - rare_cnt + 1\n","vocab_size\n","\n","print(\"학습된 총 단어의 크기 : \", vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hh5xCAI3UrhD"},"outputs":[],"source":["########################################################################\n","# 학습되는 총 단어의 크기로 다시 tokenizing\n","# 토크나이징 된 것을 pickle 파일로 저장\n","########################################################################\n","\n","total_cnt = vocab_size\n","tokenizer = Tokenizer(total_cnt)\n","tokenizer.fit_on_texts(X_train)\n","tokenizer.fit_on_texts(X_test)\n","\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPqsOlTtVZhf"},"outputs":[],"source":["########################################################################\n","# 전체적으로 문장의 길이가 어떻게 되는 지 파악하기 => padding sequence에 사용\n","########################################################################\n","\n","print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n","print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n","plt.hist([len(review) for review in X_train], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1691940003084,"user":{"displayName":"cel ee","userId":"14998685299353417937"},"user_tz":-540},"id":"jSsEALAWVyAl","outputId":"30097057-f43b-422c-d44e-9c77934e8aa8"},"outputs":[{"name":"stdout","output_type":"stream","text":["전체 샘플 중 길이가 70 이하인 샘플의 비율: 95.73231960981208\n"]}],"source":["########################################################################\n","# 전체적으로 문장의 길이가 어떻게 되는 지 파악하기 => padding sequence에 사용\n","########################################################################\n","\n","def below_threshold_len(max_len, nested_list):\n","  count = 0\n","  for sentence in nested_list:\n","    if(len(sentence) <= max_len):\n","        count = count + 1\n","  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\n","\n","max_len = 70\n","below_threshold_len(max_len, X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ygy10r9NV4Ie"},"outputs":[],"source":["X_train = pad_sequences(X_train, maxlen = max_len)\n","X_test = pad_sequences(X_test, maxlen = max_len)"]},{"cell_type":"markdown","source":["<h3> 4. 감성 분석 위해 y 라벨을 인코딩하기"],"metadata":{"id":"m6Wns_GOch7r"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MqnHxQ_4WDkx"},"outputs":[],"source":["encoder = LabelEncoder()\n","encoder.fit(y_train)\n","y_train = encoder.transform(y_train)\n","y_train = to_categorical(y_train)\n","y_test = encoder.transform(y_test)\n","y_test = to_categorical(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1691940006315,"user":{"displayName":"cel ee","userId":"14998685299353417937"},"user_tz":-540},"id":"YmYuJIVwNPRr","outputId":"0219e17e-56c0-4996-d726-d958907dd864"},"outputs":[{"data":{"text/plain":["((55768, 70), (18590, 70), (55768, 6), (18590, 6))"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"markdown","source":["<h3> 5. LSTM 모델을 이용하여 감성분석 진행하기 </h3>"],"metadata":{"id":"t-jyfC4rcl9V"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":192348,"status":"ok","timestamp":1691940207099,"user":{"displayName":"cel ee","userId":"14998685299353417937"},"user_tz":-540},"id":"2wZKF69_Wxo1","outputId":"6e10baee-2169-45d5-c4d2-dd4319635f5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","653/654 [============================>.] - ETA: 0s - loss: 1.3531 - accuracy: 0.4606\n","Epoch 1: val_accuracy improved from -inf to 0.65148, saving model to /content/drive/MyDrive/sentiment_analysis.h5\n","654/654 [==============================] - 40s 54ms/step - loss: 1.3528 - accuracy: 0.4607 - val_loss: 1.0288 - val_accuracy: 0.6515\n","Epoch 2/15\n","653/654 [============================>.] - ETA: 0s - loss: 0.9677 - accuracy: 0.6799\n","Epoch 2: val_accuracy improved from 0.65148 to 0.68333, saving model to /content/drive/MyDrive/sentiment_analysis.h5\n","654/654 [==============================] - 31s 47ms/step - loss: 0.9677 - accuracy: 0.6799 - val_loss: 0.9360 - val_accuracy: 0.6833\n","Epoch 3/15\n","653/654 [============================>.] - ETA: 0s - loss: 0.8747 - accuracy: 0.7123\n","Epoch 3: val_accuracy improved from 0.68333 to 0.68455, saving model to /content/drive/MyDrive/sentiment_analysis.h5\n","654/654 [==============================] - 30s 46ms/step - loss: 0.8746 - accuracy: 0.7124 - val_loss: 0.9276 - val_accuracy: 0.6846\n","Epoch 4/15\n","653/654 [============================>.] - ETA: 0s - loss: 0.8165 - accuracy: 0.7311\n","Epoch 4: val_accuracy did not improve from 0.68455\n","654/654 [==============================] - 30s 46ms/step - loss: 0.8165 - accuracy: 0.7311 - val_loss: 0.9247 - val_accuracy: 0.6836\n","Epoch 5/15\n","653/654 [============================>.] - ETA: 0s - loss: 0.7812 - accuracy: 0.7432\n","Epoch 5: val_accuracy did not improve from 0.68455\n","654/654 [==============================] - 30s 47ms/step - loss: 0.7809 - accuracy: 0.7433 - val_loss: 0.9523 - val_accuracy: 0.6786\n","Epoch 6/15\n","654/654 [==============================] - ETA: 0s - loss: 0.7497 - accuracy: 0.7547\n","Epoch 6: val_accuracy did not improve from 0.68455\n","654/654 [==============================] - 29s 45ms/step - loss: 0.7497 - accuracy: 0.7547 - val_loss: 0.9573 - val_accuracy: 0.6777\n","Epoch 6: early stopping\n"]}],"source":["from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","embedding_dim = 70\n","hidden_units = 64\n","num_classes = 6\n","\n","model = Sequential()\n","model.add(Embedding(total_cnt, embedding_dim))\n","model.add(LSTM(hidden_units))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))\n","chkpoint_filepath = \"/content/drive/MyDrive/sentiment_analysis.h5\"\n","\n","es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience = 3)\n","mc = ModelCheckpoint(filepath = chkpoint_filepath, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","model.compile(optimizer='adam', loss = 'categorical_crossentropy' ,metrics=['accuracy'])\n","history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.25, verbose = 1)"]},{"cell_type":"markdown","source":["<h3> 6. (번외) 머신러닝 모델을 이용하여 감성분석 모델 예측하기 </h3>\n","embedding을 하는 부분이 따로 없어서 tf-idf로 벡터화 진행 후 머신러닝 기반으로 예측<br>\n","감정 라벨을 6개로 할 경우 정확도가 17%이기 때문에, 3개로 간단하게 해서 실험"],"metadata":{"id":"zcLxqkssdTkl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ZYBxb18vNsf"},"outputs":[],"source":["import os\n","from scipy.sparse import save_npz, load_npz\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def tw_tokenizer(text):\n","    tokens_ko = okt.morphs(text)\n","    return tokens_ko\n","\n","def transform_data(x_train, x_test):\n","  if not os.path.isfile('/content/drive/MyDrive/okt_train.npz'):\n","    tfidf = TfidfVectorizer(ngram_range = (1, 2), min_df = 3, max_df = 0.9, tokenizer = okt.morphs, token_pattern = None)\n","    tfidf.fit(x_train)\n","    x_train_okt = tfidf.transform(x_train)\n","    x_test_okt = tfidf.transform(x_test)\n","    save_npz('/content/drive/MyDrive/okt_train.npz', x_train_okt)\n","    save_npz('/content/drive/MyDrive/okt_test.npz', x_test_okt)\n","  else:\n","    x_train_okt = load_npz('/content/drive/MyDrive/okt_train.npz')\n","    x_test_okt = load_npz('/content/drive/MyDrive/okt_test.npz')\n","\n","    return x_train_okt, x_test_okt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6xJ4tImy3Mt"},"outputs":[],"source":["from lightgbm import LGBMClassifier\n","from sklearn.metrics import accuracy_score\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.metrics import precision_score, recall_score\n","from sklearn.metrics import f1_score, roc_auc_score\n","import time\n","\n","def light_gbm_train(x_train_okt, x_test_okt, y_train, y_test):\n","  start = time.time()\n","  lgbm_clf = LGBMClassifier(n_estimators = 400, n_jobs = -1, verbose = 1)\n","  lgbm_clf.fit(x_train_okt, y_train)\n","  preds = lgbm_clf.predict(x_test_okt)\n","  confusion = confusion_matrix(y_test, preds)\n","  accuracy = accuracy_score(y_test, preds)\n","  precision = precision_score(y_test, preds, average='weighted')\n","  recall = recall_score(y_test, preds, average='weighted')\n","  f1 = f1_score(y_test, preds, average='weighted')\n","  print('LGBM Classifier accuracy score : ', accuracy_score(y_test, preds))\n","  print(\"LGBM Classifier\\n\")\n","  print(confusion)\n","  print('정확도: {0:.4f}\\n정밀도: {1:.4f}\\n재현율: {2:.4f}\\nf1: {3:.4f}'\n","        .format(accuracy,precision,recall,f1))\n","  print('LGBM Classifier 분류 걸린 시간 : ', time.time() - start)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxLZ9leZ0UkD"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","import time\n","\n","def LogisticRegression_train(x_train_okt, x_test_okt, y_train, y_test):\n","  start = time.time()\n","  clf = LogisticRegression(max_iter = 1000, random_state = 0)\n","  clf.fit(x_train_okt, y_train)\n","  preds = clf.predict(x_test_okt)\n","  print('LogisticRegression accuracy score : ', accuracy_score(y_test, preds))\n","  confusion = confusion_matrix(y_test, preds)\n","  accuracy = accuracy_score(y_test, preds)\n","  precision = precision_score(y_test, preds, average='weighted')\n","  recall = recall_score(y_test, preds, average='weighted')\n","  f1 = f1_score(y_test, preds, average='weighted')\n","\n","  print(\"Logistic Regression general set\\n\")\n","  print(confusion)\n","  print('정확도: {0:.4f}\\n정밀도: {1:.4f}\\n재현율: {2:.4f}\\nf1: {3:.4f}'\n","        .format(accuracy,precision,recall,f1))\n","  print('Logistic Regression 걸린 시간 : ', time.time() - start)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OABvlXos28vx"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","import time\n","\n","def LogisticRegression_liblinear(x_train_okt, x_test_okt, y_train, y_test):\n","  start = time.time()\n","  clf = LogisticRegression(max_iter = 1000, random_state = 0, solver = \"liblinear\", C = 3)\n","  clf.fit(x_train_okt, y_train)\n","  preds = clf.predict(x_test_okt)\n","  print('LogisticRegression_liblinear accuracy score : ', accuracy_score(y_test,  preds))\n","  confusion = confusion_matrix(y_test, preds)\n","  accuracy = accuracy_score(y_test, preds)\n","  precision = precision_score(y_test, preds, average='weighted')\n","  recall = recall_score(y_test, preds, average='weighted')\n","  f1 = f1_score(y_test, preds, average='weighted')\n","\n","  print(\"Logistic Regression liblinear\")\n","  print(confusion)\n","  print('정확도: {0:.4f}\\n정밀도: {1:.4f}\\n재현율: {2:.4f}\\nf1: {3:.4f}'\n","        .format(accuracy,precision,recall,f1))\n","  print('Logistic Regression liblinear 걸린 시간 : ', time.time() - start)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFrm01uF05Hi"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","import time\n","\n","def Logistic_with_CountVectorizer(x_data, y_data):\n","  start = time.time()\n","  count_vec = CountVectorizer(tokenizer = okt.morphs)\n","  x_train_data = count_vec.fit_transform(x_data)\n","  x_train, x_test, y_train, y_test = train_test_split(x_train_data, y_data, test_size = 0.25, random_state = 156)\n","  clf = LogisticRegression(max_iter = 1000, solver = 'liblinear',)\n","  clf.fit(x_train, y_train)\n","  preds = clf.predict(x_test)\n","  print('Logistic with CountVectorizer accuracy score : ', accuracy_score(y_test, preds))\n","  confusion = confusion_matrix(y_test, preds)\n","  accuracy = accuracy_score(y_test, preds)\n","  precision = precision_score(y_test, preds, average='weighted')\n","  recall = recall_score(y_test, preds, average='weighted')\n","  f1 = f1_score(y_test, preds, average='weighted')\n","\n","  print(\"Logistic with CountVectorizer\\n\")\n","  print(confusion)\n","  print('정확도: {0:.4f}\\n정밀도: {1:.4f}\\n재현율: {2:.4f}\\nf1: {3:.4f}'\n","        .format(accuracy,precision,recall,f1))\n","  print('Logistic with CountVectorizer 걸린 시간 : ', time.time() - start)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"w1PV9M5rkpfd","outputId":"1e1a6e46-0082-4908-c467-976338baa414"},"outputs":[{"name":"stdout","output_type":"stream","text":["===================전체 정확도=================\n","[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 16.535293 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 840042\n","[LightGBM] [Info] Number of data points in the train set: 55768, number of used features: 24407\n","[LightGBM] [Info] Start training from score -1.793339\n","[LightGBM] [Info] Start training from score -1.101000\n","[LightGBM] [Info] Start training from score -0.691034\n","LGBM Classifier accuracy score :  0.470575578267886\n","LGBM Classifier\n","\n","[[  43  505 2545]\n"," [  70 1019 5093]\n"," [ 124 1505 7686]]\n","정확도: 0.4706\n","정밀도: 0.3934\n","재현율: 0.4706\n","f1: 0.3905\n","LGBM Classifier 분류 걸린 시간 :  415.4382266998291\n","LogisticRegression accuracy score :  0.464228079612695\n","Logistic Regression general set\n","\n","[[   9  563 2521]\n"," [  26 1061 5095]\n"," [  35 1720 7560]]\n","정확도: 0.4642\n","정밀도: 0.3765\n","재현율: 0.4642\n","f1: 0.3844\n","Logistic Regression 걸린 시간 :  47.598690032958984\n","LogisticRegression_liblinear accuracy score :  0.4521785906401291\n","Logistic Regression liblinear\n","[[  49  747 2297]\n"," [ 116 1368 4698]\n"," [ 150 2176 6989]]\n","정확도: 0.4522\n","정밀도: 0.3823\n","재현율: 0.4522\n","f1: 0.3923\n","Logistic Regression liblinear 걸린 시간 :  10.674432516098022\n"]}],"source":["from konlpy.tag import Okt\n","from lightgbm import LGBMClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import os\n","from scipy.sparse import save_npz, load_npz\n","\n","\n","train_df['text'] = train_df['사람문장1'] + train_df['시스템응답1'] + train_df['사람문장2'].map(str) + train_df['시스템응답2'] + train_df['사람문장3'] + train_df['시스템응답3']\n","x_data =  train_df['text'].apply(lambda x : re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]', \" \", x)).values\n","y_data = train_df['label']\n","x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, stratify = y_data, test_size = 0.25, random_state = 42)\n","\n","okt = Okt()\n","\n","if not os.path.isfile('second_train.npz'):\n","  tfidf = TfidfVectorizer(ngram_range = (1, 2), min_df = 3, max_df = 0.9, tokenizer = okt.morphs, token_pattern = None)\n","  tfidf.fit(x_train)\n","  x_train_okt = tfidf.transform(x_train)\n","  x_test_okt = tfidf.transform(x_test)\n","  save_npz('second_train.npz', x_train_okt)\n","  save_npz('second_test.npz', x_test_okt)\n","else:\n","  x_train_okt = load_npz('second_train.npz')\n","  x_test_okt = load_npz('second_test.npz')\n","\n","print('===================전체 정확도=================')\n","\n","encoder = LabelEncoder()\n","encoder.fit(y_train)\n","y_train = encoder.fit_transform(y_train)\n","\n","encoder = LabelEncoder()\n","encoder.fit(y_test)\n","y_test = encoder.fit_transform(y_test)\n","\n","X_train, X_test = transform_data(x_train_okt, x_test_okt)\n","light_gbm_train(X_train, X_test, y_train, y_test)\n","LogisticRegression_train(X_train, X_test, y_train, y_test)\n","LogisticRegression_liblinear(X_train, X_test, y_train, y_test)\n","#Logistic_with_CountVectorizer(x_data, y_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xn90c9FwOF3"},"outputs":[],"source":["text = [\"나 진짜 너 오늘 혼냄 ㅋ\", \"그 영화 진짜 너무 재밌었던 것 같아서 행복해\",\n","        \"요즘 인생 현타 겁나 옴 ㅋ\", \"나 오늘 시험 100점 맞을 겨\", \"짜증나\", \"나는 뭘 해도 안될거야\", \"너 때문에 내가 지금 너무 화난다.\"]\n","clf = LogisticRegression(max_iter = 1000, random_state = 0, solver = \"liblinear\", C = 3)\n","clf.fit(x_train_okt, y_train)\n","score = clf.predict(tfidf.transform(text))\n","print(score)\n","print(clf.predict_proba(tfidf.transform(text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJpwUY_cwFPL"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEOTZOhZy3H+DRxfmMhr4J"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}